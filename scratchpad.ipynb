{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "#     Sum_of_squared_distances = []\n",
    "#     K = range(2,100)\n",
    "#     for k in K:\n",
    "#        km = KMeans(n_clusters=k, max_iter=200, n_init=10)\n",
    "#        km = km.fit(X)\n",
    "#        Sum_of_squared_distances.append(km.inertia_)\n",
    "#     plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "#     plt.xlabel('k')\n",
    "#     plt.ylabel('Sum_of_squared_distances')\n",
    "#     plt.title('Elbow Method For Optimal k')\n",
    "#     plt.show()\n",
    "\n",
    "    # print('How many clusters do you want to use?')\n",
    "    # true_k = int(input())\n",
    "    # predict which cluster newWord belongs to\n",
    "    # newWord_cluster = model.predict(vectorizer.transform([newWord]))\n",
    "#     import numpy as np\n",
    "# from sklearn.cluster import AffinityPropagation\n",
    "# import distance\n",
    "\n",
    "# def analyse_1(data):\n",
    "#     words = np.asarray(data) #So that indexing with a list will work\n",
    "#     lev_similarity = -1*np.array([[distance.levenshtein(w1,w2) for w1 in words] for w2 in words])\n",
    "\n",
    "#     affprop = AffinityPropagation(affinity=\"precomputed\", damping=0.5)\n",
    "#     affprop.fit(lev_similarity)\n",
    "#     for cluster_id in np.unique(affprop.labels_):\n",
    "#         exemplar = words[affprop.cluster_centers_indices_[cluster_id]]\n",
    "#         cluster = np.unique(words[np.nonzero(affprop.labels_==cluster_id)])\n",
    "#         cluster_str = \", \".join(cluster)\n",
    "#         print(\" - *%s:* %s\" % (exemplar, cluster_str))\n",
    "    \n",
    "# import nltk\n",
    "# import pandas\n",
    "# import pprint\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn import cluster\n",
    "# from sklearn import metrics\n",
    "# from gensim.models import Word2Vec\n",
    "# from nltk.cluster import KMeansClusterer\n",
    "# from sklearn.metrics import adjusted_rand_score\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from sklearn.decomposition import NMF\n",
    "\n",
    "# def cluster_word_to_evac(word_list):\n",
    "#     min_count = 2\n",
    "#     size = 500\n",
    "#     window = 4\n",
    "\n",
    "#     model = Word2Vec(word_list, min_count=min_count, vector_size=size, window=window)\n",
    "#     X = np.asarray(model.wv.vectors)\n",
    "# #     labels = np.asarray(model.wv.index_to_key) \n",
    "# #     X = model[model.wv.index_to_key]\n",
    "\n",
    "#     print(len(X))\n",
    "\n",
    "#     clusters_number = 26\n",
    "    # kclusterer = KMeansClusterer(clusters_number,  distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "\n",
    "    # assigned_clusters = kclusterer.cluster(X, assign_clusters=True)\n",
    "\n",
    "    # words = list(model.wv.index_to_key)\n",
    "    # for i, word in enumerate(words):  \n",
    "    #     print (word + \":\" + str(assigned_clusters[i]))\n",
    "\n",
    "    # kmeans = cluster.KMeans(n_clusters = clusters_number)\n",
    "    # kmeans.fit(X)\n",
    "\n",
    "    # labels = kmeans.labels_\n",
    "    # centroids = kmeans.cluster_centers_\n",
    "\n",
    "    # clusters = {}\n",
    "    # score = silhouette_score(X, labels, metric='euclidean')\n",
    "\n",
    "    # print('Silhouetter Score: %.3f' % score)\n",
    "    # for commentaires, label in zip(word_list, labels):\n",
    "    #     try:\n",
    "    #         clusters[str(label)].append(commentaires)\n",
    "    #     except:\n",
    "    #        clusters[str(label)] = [commentaires]\n",
    "    # pprint.pprint(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from services.database import ActivityClusterDatabaseService,ActivityUserDatabaseService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Database Service\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'activityId': 'string', 'userId': 'string', 'score': 1}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ActivityClusterDatabaseService().get_all()\n",
    "ActivityUserDatabaseService().get_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../vectorizer.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/habib/Desktop/aait/ultimate_feed_nlp/scratchpad.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/habib/Desktop/aait/ultimate_feed_nlp/scratchpad.ipynb#ch0000003?line=8'>9</a>\u001b[0m     model\u001b[39m=\u001b[39m joblib\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mmodel.pkl\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/habib/Desktop/aait/ultimate_feed_nlp/scratchpad.ipynb#ch0000003?line=9'>10</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39mpredict(Y)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/habib/Desktop/aait/ultimate_feed_nlp/scratchpad.ipynb#ch0000003?line=11'>12</a>\u001b[0m predictCluster(\u001b[39m\"\u001b[39;49m\u001b[39mhello\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/Users/habib/Desktop/aait/ultimate_feed_nlp/scratchpad.ipynb Cell 4'\u001b[0m in \u001b[0;36mpredictCluster\u001b[0;34m(feature)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/habib/Desktop/aait/ultimate_feed_nlp/scratchpad.ipynb#ch0000003?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredictCluster\u001b[39m(feature:\u001b[39mstr\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/habib/Desktop/aait/ultimate_feed_nlp/scratchpad.ipynb#ch0000003?line=5'>6</a>\u001b[0m     vectorizer \u001b[39m=\u001b[39mjoblib\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39m../vectorizer.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/habib/Desktop/aait/ultimate_feed_nlp/scratchpad.ipynb#ch0000003?line=6'>7</a>\u001b[0m     \u001b[39m# # predict clust of text using the model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/habib/Desktop/aait/ultimate_feed_nlp/scratchpad.ipynb#ch0000003?line=7'>8</a>\u001b[0m     Y \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mtransform([feature])\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/joblib/numpy_pickle.py:579\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/habib/Library/Python/3.8/lib/python/site-packages/joblib/numpy_pickle.py?line=576'>577</a>\u001b[0m         obj \u001b[39m=\u001b[39m _unpickle(fobj)\n\u001b[1;32m    <a href='file:///Users/habib/Library/Python/3.8/lib/python/site-packages/joblib/numpy_pickle.py?line=577'>578</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/habib/Library/Python/3.8/lib/python/site-packages/joblib/numpy_pickle.py?line=578'>579</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(filename, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    <a href='file:///Users/habib/Library/Python/3.8/lib/python/site-packages/joblib/numpy_pickle.py?line=579'>580</a>\u001b[0m         \u001b[39mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[39mas\u001b[39;00m fobj:\n\u001b[1;32m    <a href='file:///Users/habib/Library/Python/3.8/lib/python/site-packages/joblib/numpy_pickle.py?line=580'>581</a>\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fobj, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/habib/Library/Python/3.8/lib/python/site-packages/joblib/numpy_pickle.py?line=581'>582</a>\u001b[0m                 \u001b[39m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/habib/Library/Python/3.8/lib/python/site-packages/joblib/numpy_pickle.py?line=582'>583</a>\u001b[0m                 \u001b[39m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/habib/Library/Python/3.8/lib/python/site-packages/joblib/numpy_pickle.py?line=583'>584</a>\u001b[0m                 \u001b[39m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../vectorizer.pkl'"
     ]
    }
   ],
   "source": [
    "from services.cluster_features import predictCluster\n",
    "import joblib;\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def predictCluster(feature:str):\n",
    "    vectorizer =joblib.load('vectorizer.pkl')\n",
    "    # # predict clust of text using the model\n",
    "    Y = vectorizer.transform([feature])\n",
    "    model= joblib.load('model.pkl')\n",
    "    return model.predict(Y)\n",
    "\n",
    "predictCluster(\"hello\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
