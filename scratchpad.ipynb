{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "#     Sum_of_squared_distances = []\n",
    "#     K = range(2,100)\n",
    "#     for k in K:\n",
    "#        km = KMeans(n_clusters=k, max_iter=200, n_init=10)\n",
    "#        km = km.fit(X)\n",
    "#        Sum_of_squared_distances.append(km.inertia_)\n",
    "#     plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "#     plt.xlabel('k')\n",
    "#     plt.ylabel('Sum_of_squared_distances')\n",
    "#     plt.title('Elbow Method For Optimal k')\n",
    "#     plt.show()\n",
    "\n",
    "    # print('How many clusters do you want to use?')\n",
    "    # true_k = int(input())\n",
    "    # predict which cluster newWord belongs to\n",
    "    # newWord_cluster = model.predict(vectorizer.transform([newWord]))\n",
    "#     import numpy as np\n",
    "# from sklearn.cluster import AffinityPropagation\n",
    "# import distance\n",
    "\n",
    "# def analyse_1(data):\n",
    "#     words = np.asarray(data) #So that indexing with a list will work\n",
    "#     lev_similarity = -1*np.array([[distance.levenshtein(w1,w2) for w1 in words] for w2 in words])\n",
    "\n",
    "#     affprop = AffinityPropagation(affinity=\"precomputed\", damping=0.5)\n",
    "#     affprop.fit(lev_similarity)\n",
    "#     for cluster_id in np.unique(affprop.labels_):\n",
    "#         exemplar = words[affprop.cluster_centers_indices_[cluster_id]]\n",
    "#         cluster = np.unique(words[np.nonzero(affprop.labels_==cluster_id)])\n",
    "#         cluster_str = \", \".join(cluster)\n",
    "#         print(\" - *%s:* %s\" % (exemplar, cluster_str))\n",
    "    \n",
    "# import nltk\n",
    "# import pandas\n",
    "# import pprint\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn import cluster\n",
    "# from sklearn import metrics\n",
    "# from gensim.models import Word2Vec\n",
    "# from nltk.cluster import KMeansClusterer\n",
    "# from sklearn.metrics import adjusted_rand_score\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from sklearn.decomposition import NMF\n",
    "\n",
    "# def cluster_word_to_evac(word_list):\n",
    "#     min_count = 2\n",
    "#     size = 500\n",
    "#     window = 4\n",
    "\n",
    "#     model = Word2Vec(word_list, min_count=min_count, vector_size=size, window=window)\n",
    "#     X = np.asarray(model.wv.vectors)\n",
    "# #     labels = np.asarray(model.wv.index_to_key) \n",
    "# #     X = model[model.wv.index_to_key]\n",
    "\n",
    "#     print(len(X))\n",
    "\n",
    "#     clusters_number = 26\n",
    "    # kclusterer = KMeansClusterer(clusters_number,  distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "\n",
    "    # assigned_clusters = kclusterer.cluster(X, assign_clusters=True)\n",
    "\n",
    "    # words = list(model.wv.index_to_key)\n",
    "    # for i, word in enumerate(words):  \n",
    "    #     print (word + \":\" + str(assigned_clusters[i]))\n",
    "\n",
    "    # kmeans = cluster.KMeans(n_clusters = clusters_number)\n",
    "    # kmeans.fit(X)\n",
    "\n",
    "    # labels = kmeans.labels_\n",
    "    # centroids = kmeans.cluster_centers_\n",
    "\n",
    "    # clusters = {}\n",
    "    # score = silhouette_score(X, labels, metric='euclidean')\n",
    "\n",
    "    # print('Silhouetter Score: %.3f' % score)\n",
    "    # for commentaires, label in zip(word_list, labels):\n",
    "    #     try:\n",
    "    #         clusters[str(label)].append(commentaires)\n",
    "    #     except:\n",
    "    #        clusters[str(label)] = [commentaires]\n",
    "    # pprint.pprint(clusters)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
